{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "README\n",
    "William Ian McKinely\n",
    "MSBI 31600 Advanced Project Winter 2024\n",
    "\n",
    "1. Description of the project\n",
    "This project is an initial framework for a pipeline which can take in a number of audio files which will be recordings from \n",
    "operating rooms collected for a research project examining teacher-learner verbal interactions. This uses the AWS Transcribe \n",
    "Medical service to transcribe the audio files into a JSON file, which can be pulled from AWS\n",
    "servers and parsed into a formatted transcript which can be collected as a dataframe that can be passed to a downstream\n",
    "analysis. \n",
    "\n",
    "2. Why I selected the project\n",
    "I am the PI for this study, and I value the ability to plan for a long-term functional database of transcripts for analysis.\n",
    "I think that we will begin to identify verbal patterns that we currently don't describe and which are unique to surgery, so\n",
    "having something with a longstanding structure will be important when we need to include something retroactively. Also I'm\n",
    "secretly super skeptical of the ability for AI to outperform humans in transcription, especially in our very specific use case\n",
    "that is probably otherwise not well represented in the training data, so I was curious to explore it. Currently, the project\n",
    "suffers from a limitation caused by the upper limits of capacity of human transcriptionists, which is compounded by the cost;\n",
    "using AI would be a lot cheaper, but maybe not better in the long run.\n",
    "\n",
    "I have learned a lot about the need for incredibly clean data in order for use of AI to be successful. Our audio files \n",
    "(the research files, which are not provided here) are real-world audio; they contain overlap, background noise, and \n",
    "other elements which easily confuse the software. To be fair, they often also confuse humans, but the humans know to \n",
    "notice it and find clarification or otherwise adjsut. I also have a much greater appreciation for anyone who makes something\n",
    "that passes between systems because it is quite challenging. We will need to do a lot of work before this will be a viable\n",
    "solution to our problem, but it was enough to get me interested, and I have a couple of ideas about where to go next \n",
    "(two channel audio input, custom vocabulary). \n",
    "\n",
    "3. Themes selected\n",
    "Theme 1: JSON\n",
    "I used JSON.loads() to get the data from a stream into a local variable; I also broke it in about every way you can imagine,\n",
    "which is not shown here because I care about your happiness. I used the structure of the JSON to my advantage to reliably\n",
    "parse it to a dataframe representing my desired format. Choosing this was almost a default choice; the organization structure\n",
    "means I will almost certainly have a lot of my future occupied in the JSON format, so it would seem crazy to have possibly \n",
    "made a different choice.\n",
    "Theme 2: Server/client relationships & APIs\n",
    "I interfaced with the AWS API using the boko3 package. I selected this theme because I plan to do a lot of work with large\n",
    "data sets in the future; this will inevitably mean use of a cloud service or remote HPC, and either will function along\n",
    "similar lines so mastering navigation of this relationship is prudent. These elements are applied in the class declaration\n",
    "for 'server_grab' because this is what connects to the AWS servers, starts the transcription job, and then pulls the data\n",
    "in through a data stream.\n",
    "\n",
    "4. What I would do differently\n",
    "First, I would have liked to make this a much more in-depth exploration of the elements. However, limitations stemming\n",
    "from both real life and the nature of writing code meant that this wasn't always possible or realistic. I would have really\n",
    "liked to make a UI that allows the user to pick the files to be transcribed (currently you have to either manually list them,\n",
    "have a folder for them which gets selected and then you go move them out manually later, or something like that) so that\n",
    "more of the recordings could be stored in a single location but not all be transcribed repeatedly. I would also probably\n",
    "have started versioning earlier, so that I could build a framework and piece it together. Instead, I have a smattering\n",
    "of files which represent me going through the pieces and then a master where I eventually put them together. \n",
    "\n",
    "5. How to run the project\n",
    "***\n",
    "\n",
    "6. Challenges & Obstacles\n",
    "This project was definitely challenging in the way I expected. I knew that I fundamentally would be able to make the JSON \n",
    "parsing work out, but it wasn't always easy to get the time modules to cooperate. I really did not enjoy engaging the AWS\n",
    "API, and I found it very confusing to start but I eventually found my footing. I was able to answer a lot of questions for\n",
    "myself about what is realistically possible, and was also able to determine which elements we will still need to work to \n",
    "overcome in the long term that can't be fixed programatically (like having 2 channel audio input, for example).\n",
    "\n",
    "7. Sources\n",
    "Sources are cited in-line throughout the code wherever they were used. I also used the AWS documentation for the Transcribe\n",
    "Medical service to get me moving. \n",
    "\n",
    "8. Extra credit\n",
    "***\n",
    "I met the criteria for using try/except in the JSON parsing cell while setting the first starttime.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import datetime\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = dict()\n",
    "full_data = pd.DataFrame(columns=['spk','start_time','end_time','transcript'])\n",
    "\n",
    "@dataclass\n",
    "class server_grab():\n",
    "\n",
    "    full_data = pd.DataFrame(columns=['spk','start_time','end_time','transcript'])\n",
    "\n",
    "    def __init__(self):\n",
    "        self.spk:str = \"spk_0\"\n",
    "        self.start_time = datetime.timedelta(days=0,seconds=0,milliseconds=0)\n",
    "        self.end_time = datetime.timedelta(days=0,seconds=0,milliseconds=0)\n",
    "        self.transcript:str = \"\"\n",
    "        self.s3_client = boto3.client('s3')\n",
    "        self.client = boto3.client('transcribe')\n",
    "        self.s3_resource = boto3.resource('s3')\n",
    "        bucket = self.s3_resource.Bucket(name='msbi31600')\n",
    "\n",
    "    #Here we can submit the job to the transcription service\n",
    "    #Future improvements would include dynamic file naming, selection for output locations, and multiple file handling\n",
    "    #A better version would also add error handling for job submissions\n",
    "    def sub_job(self):\n",
    "        job = self.client.start_medical_transcription_job(\n",
    "            MedicalTranscriptionJobName='test_job',\n",
    "            LanguageCode='en-US',\n",
    "            MediaFormat='m4a',\n",
    "            Media={\n",
    "                'MediaFileUri': 's3://msbi31600/test_audio.m4a'\n",
    "            },\n",
    "            OutputBucketName='msbi31600',\n",
    "         #OutputKey='outputs/test_output2.json',\n",
    "            Settings={\n",
    "        'ShowSpeakerLabels': True,\n",
    "        'MaxSpeakerLabels': 2,\n",
    "        'ChannelIdentification': False\n",
    "            },\n",
    "            Specialty='PRIMARYCARE',\n",
    "            Type = 'CONVERSATION'\n",
    "        )\n",
    "    \n",
    "    #This function gets the transcription JSON file from the bucket and puts it into a variable for our use\n",
    "    def get_json(self):\n",
    "        data = self.s3_client.get_object(Bucket='msbi31600',Key='medical/test_job.json')\n",
    "        body=data['Body']\n",
    "        j = json.loads(body.read().decode(\"utf-8\"))\n",
    "        return j\n",
    "        \"\"\"\n",
    "        results = self.s3_client.list_objects(Bucket='msbi31600', Prefix='medical/test_job')\n",
    "        for file in results.get('Contents'):\n",
    "            data = self.s3_client.get_object(Bucket='msbi31600', Key=file.get('Key'))\n",
    "            contents = data['Body'].read()\n",
    "            j = json.loads(contents,parse_float='Decimal')\n",
    "            return j\n",
    "        print(j)\n",
    "        globals()[dvar] = j\"\"\"\n",
    "\n",
    "    #This function deletes the job if you encounter the error that the job already exists\n",
    "    #Future error handling might be able to catch this error and delete the job automatically, but it would have to be a precise situation because this could cause big problems\n",
    "    def del_job(self):\n",
    "        dummy = self.client.delete_medical_transcription_job(\n",
    "            MedicalTranscriptionJobName='test_job'\n",
    "        )\n",
    "\n",
    "class file_parser():\n",
    "\n",
    "    def __init__(self, file):\n",
    "        self.file = file\n",
    "        self.start_time = datetime.timedelta(days=0,seconds=0,milliseconds=0)\n",
    "        self.end_time = datetime.timedelta(days=0,seconds=0,milliseconds=0)\n",
    "        self.second_start=int()\n",
    "        self.minute_start=int()\n",
    "        self.second_end=int()\n",
    "        self.minute_end=int()\n",
    "        self.spk = \"spk_0\"\n",
    "        self.transcript = \"\"\n",
    "\n",
    "    def parse(self):\n",
    "        global full_data\n",
    "        #This segment is the broadest loop, which is pulling each 'item' from the Amazon-produced JSON file and parsing it to the useful chunks we need\n",
    "        for item in self.file['results']['items']:\n",
    "    \n",
    "            #This block checks if the speaker label has changed at its outermost scope, and behaves differently depending on whether or not the speaker has changed\n",
    "            #We may need to change the behavior at this level when we start using multi channel input in the future\n",
    "            if item['speaker_label'] == self.spk:\n",
    "        \n",
    "                #First we have to see if the item is a punctuation mark, and if so, we need to add it to the transcript without a space and without changing the timestamp\n",
    "                if item['type'] == 'punctuation':\n",
    "                    self.transcript = self.transcript + item['alternatives'][0]['content']\n",
    "                elif item['type'] == 'pronunciation':\n",
    "\n",
    "                    #If the speaker hasn't changed, we add the content to the transcript\n",
    "                    #I plan to come back here later and add a checking function, where if the item type is 'punctuation' it will omit the added space before addending the content\n",
    "                    self.transcript = self.transcript + \" \" + item['alternatives'][0]['content']\n",
    "        \n",
    "                    while True:\n",
    "                        try:\n",
    "                            #We also check to see if the start time is listed as being 0.0 (i.e. if it has been reset) and if so, we set it to the first value we come across since the reset\n",
    "                            #This allows us to set the correct start time for each line of text per speaker, which will be important for assigning pauses in the future\n",
    "                            if self.start_time.total_seconds() == 0.0:\n",
    "                                split_start = item['start_time'].split('.')\n",
    "                                second_start = int(split_start[0])\n",
    "                                millisecond_start = (split_start[1])\n",
    "                                if len(millisecond_start) == 1:\n",
    "                                    millisecond_start = int(millisecond_start) * 100\n",
    "                                elif len(millisecond_start) == 2:\n",
    "                                    millisecond_start = int(millisecond_start) * 10\n",
    "                                else:\n",
    "                                    millisecond_start = int(millisecond_start)\n",
    "                                start_time_hold = datetime.timedelta(days=0,seconds=second_start,milliseconds=millisecond_start)\n",
    "                \n",
    "                                #Lifted this directly from https://stackoverflow.com/a/539360 & modified it slightly for own use\n",
    "                                s1= start_time_hold.total_seconds()\n",
    "                                hours1, remain1 = divmod(s1, 3600)\n",
    "                                minutes1, remain2_1 = divmod(remain1, 60)\n",
    "                                #Remember that divisor on the following line is 1, because the whole thing is indexed to seconds\n",
    "                                seconds1, remain3_1 = divmod(remain2_1, 1)\n",
    "                                #We added this modifier to increase the remainder so the milliseconds format correctly\n",
    "                                remain3_1 = remain3_1 * 1000\n",
    "                                var1 = ('{:02}:{:02}:{:02}:{:03}'.format(int(hours1), int(minutes1), int(seconds1), int(remain3_1)))\n",
    "\n",
    "                                self.start_time = var1\n",
    "                                break\n",
    "                            else:\n",
    "                                break\n",
    "                        except AttributeError:\n",
    "                            break\n",
    "        \n",
    "                    #This block takes the end time from the JSON file and parses it into a timedelta object by splitting it and then putting it back together in the desired format\n",
    "                    split_end = item['end_time'].split('.')\n",
    "                    second_end = int(split_end[0])\n",
    "                    millisecond_end = (split_end[1])\n",
    "                    if len(millisecond_end) == 1:\n",
    "                        millisecond_end = int(millisecond_end) * 100\n",
    "                    elif len(millisecond_end) == 2:\n",
    "                        millisecond_end = int(millisecond_end) * 10\n",
    "                    else:\n",
    "                        millisecond_end = int(millisecond_end)\n",
    "                    end_time_hold = datetime.timedelta(days=0,seconds=second_end,milliseconds=millisecond_end)\n",
    "            \n",
    "                    #Lifted this directly from https://stackoverflow.com/a/539360 & modified it slightly for own use\n",
    "                    s2= end_time_hold.total_seconds()\n",
    "                    hours2, remain2 = divmod(s2, 3600)\n",
    "                    minutes2, remain2_2 = divmod(remain2, 60)\n",
    "                    #Remember that divisor on the following line is 1, because the whole thing is indexed to seconds\n",
    "                    seconds2, remain3_2 = divmod(remain2_2, 1)\n",
    "                    #We added this modifier to increase the remainder so the milliseconds format correctly\n",
    "                    remain3_2 = remain3_2 * 1000\n",
    "                    var2 = ('{:02}:{:02}:{:02}:{:03}'.format(int(hours2), int(minutes2), int(seconds2), int(remain3_2)))\n",
    "            \n",
    "                    #Then we replace the end time with the holder, to be sure it is updated every time and no failures will occur\n",
    "                    self.end_time = var2\n",
    "    \n",
    "            #This is the block which is called if the speaker changes\n",
    "            elif item['speaker_label'] != self.spk:\n",
    "        \n",
    "                #We add the data currently in our containers to the dataframe\n",
    "                full_data = full_data.append({'spk':self.spk,'start_time':self.start_time,'end_time':self.end_time,'transcript':self.transcript},ignore_index=True)\n",
    "        \n",
    "                #Change the speaker label to the new speaker\n",
    "                self.spk = item['speaker_label']\n",
    "        \n",
    "                #Reset the start time to the current value\n",
    "                split_start = item['start_time'].split('.')\n",
    "                second_start = int(split_start[0])\n",
    "                millisecond_start = (split_start[1])\n",
    "                if len(millisecond_start) == 1:\n",
    "                    millisecond_start = int(millisecond_start) * 100\n",
    "                elif len(millisecond_start) == 2:\n",
    "                    millisecond_start = int(millisecond_start) * 10\n",
    "                else:\n",
    "                    millisecond_start = int(millisecond_start)\n",
    "                start_time_hold = datetime.timedelta(days=0,seconds=second_start,milliseconds=millisecond_start)\n",
    "        \n",
    "                #Lifted this directly from https://stackoverflow.com/a/539360 & modified it slightly for own use\n",
    "                s3= start_time_hold.total_seconds()\n",
    "                hours3, remain3 = divmod(s3, 3600)\n",
    "                minutes3, remain2_3 = divmod(remain3, 60)\n",
    "                #Remember that divisor on the following line is 1, because the whole thing is indexed to seconds\n",
    "                seconds3, remain3_3 = divmod(remain2_3, 1)\n",
    "                #We added this modifier to increase the remainder so the milliseconds format correctly\n",
    "                remain3_3 = remain3_3 * 1000\n",
    "                var3 = ('{:02}:{:02}:{:02}:{:03}'.format(int(hours3), int(minutes3), int(seconds3), int(remain3_3)))\n",
    "                self.start_time = var3\n",
    "\n",
    "                #This block takes the end time from the JSON file and parses it into a timedelta object by splitting it and then putting it back together in the desired format\n",
    "                split_end = item['end_time'].split('.')\n",
    "                second_end = int(split_end[0])\n",
    "                millisecond_end = (split_end[1])\n",
    "                if len(millisecond_end) == 1:\n",
    "                    millisecond_end = int(millisecond_end) * 100\n",
    "                elif len(millisecond_end) == 2:\n",
    "                    millisecond_end = int(millisecond_end) * 10\n",
    "                else:\n",
    "                    millisecond_end = int(millisecond_end)\n",
    "                end_time_hold = datetime.timedelta(days=0,seconds=second_end,milliseconds=millisecond_end)\n",
    "        \n",
    "                #Lifted this directly from https://stackoverflow.com/a/539360 & modified it slightly for own use\n",
    "                sx= end_time_hold.total_seconds()\n",
    "                hoursx, remainx = divmod(sx, 3600)\n",
    "                minutesx, remainx2 = divmod(remainx, 60)\n",
    "                #Remember that divisor on the following line is 1, because the whole thing is indexed to seconds\n",
    "                secondsx, remainx3 = divmod(remainx2, 1)\n",
    "                #We added this modifier to increase the remainder so the milliseconds format correctly\n",
    "                remainx3 = remainx3 * 1000\n",
    "                var4 = ('{:02}:{:02}:{:02}:{:03}'.format(int(hoursx), int(minutesx), int(secondsx), int(remainx3)))\n",
    "                self.end_time = var4\n",
    "\n",
    "                #Reset the transcript to be blank\n",
    "                self.transcript = \"\"\n",
    "                #Then add the current content to the transcript\n",
    "                self.transcript = self.transcript + \" \" + item['alternatives'][0]['content']\n",
    "\n",
    "        #When the whole thing has run, the final line of data will not have been added to the dataframe, so we add it here\n",
    "        full_data = full_data.append({'spk':self.spk,'start_time':self.start_time,'end_time':self.end_time,'transcript':self.transcript},ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "mine = server_grab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#After you submit the job, you have to wait a while for the transcription to complete\n",
    "#I need to add something to check the status of the job to enhance the user experience\n",
    "mine.sub_job()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is a functional solution to getting the file out; it is inelegant, but good enough for government work\n",
    "my_file = mine.get_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell instantiates the file parser and passes 'file' into it\n",
    "myp = file_parser(my_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell parses the JSON file into the dataframe\n",
    "myp.parse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
